{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在模型训练或预测时，我们常常会同时处理多个数据样本并用到矢量计算\n",
    "# 在介绍线性回归的矢量计算表达式之前，让我们先考虑对两个向量相加的两种方法\n",
    "# 下面先定义两个1000维的向量\n",
    "import torch\n",
    "from time import time\n",
    "\n",
    "a = torch.ones(1000)\n",
    "b = torch.ones(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0061969757080078125\n"
     ]
    }
   ],
   "source": [
    "# 向量相加的一种方法是，将这两个向量按元素逐一做标量加法\n",
    "start = time()\n",
    "c = torch.zeros(1000)\n",
    "for i in range(1000):\n",
    "    c[i] = a[i] + b[i]\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 向量相加的另一种方法是，将这两个向量直接做矢量加法\n",
    "start = time()\n",
    "d = a + b\n",
    "print(time() - start)\n",
    "# 结果很明显，后者比前者更省时。因此，我们应该尽可能采用矢量计算，以提升计算效率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11., 11., 11.])\n"
     ]
    }
   ],
   "source": [
    "# 重新复习广播机制\n",
    "# 当对两个形状不同的张量按元素运算时，可能会触发广播机制：先适当复制元素使这两个张量形状相同后再按元素运算\n",
    "a = torch.ones(3)\n",
    "b = 10\n",
    "print(a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫作解析解（analytical solution）。本节使用的线性回归和平方误差刚好属于这个范畴。然而，大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作数值解（numerical solution）。\n",
    "\n",
    "在求数值解的优化算法中，小批量随机梯度下降（mini-batch stochastic gradient descent）在深度学习中被广泛使用。它的算法很简单：先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch）B，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。\n",
    "\n",
    "∣B∣ 代表每个小批量中的样本个数（批量大小，batch size）, η 称作学习率（learning rate）并取正数。需要强调的是，这里的批量大小和学习率的值是人为设定的，并不是通过模型训练学出的，因此叫作超参数（hyperparameter）。我们通常所说的“调参”指的正是调节超参数，例如通过反复试错来找到超参数合适的值。在少数情况下，超参数也可以通过模型训练学出。本书对此类情况不做讨论。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
