{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动求导计算一个函数在指定值上的导数\n",
    "# 计算图：将代码分解成操作子，将计算表示成一个无环图\n",
    "# 自动求导的两种模式：\n",
    "# ①正向累积：需要存储正向的所有结果，内存复杂度O(n)；②反向传播，又称反向传递，内存复杂度O(1)，后者更常用\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''求梯度就是求导数'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 假设我们想对函数y = 2 * xT * x求关于列向量x求导\n",
    "x = torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在我们计算y关于的梯度之前，我们需要一个地方来存储梯度\n",
    "x.requires_grad_(True)  # 等价于x = torch.arange(4.0, requires_grad=True)\n",
    "# 这意味着在后续的计算中，PyTorch 将会跟踪所有对 x 的操作，以便在反向传播时自动计算梯度\n",
    "# 在计算梯度之前，x.grad 默认是 None，在反向传播计算梯度之后，x.grad 将会存储 x 的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(28., grad_fn=<MulBackward0>),\n",
       " tensor(56., grad_fn=<MulBackward0>),\n",
       " tensor(3136., grad_fn=<PowBackward0>))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 现在让我们计算y(x和x内积乘以2), z(2y), h(z的平方)\n",
    "y = 2 * torch.dot(x, x)\n",
    "y.retain_grad()  # 保留y的梯度\n",
    "z = 2 * y\n",
    "z.retain_grad()  # 保留z的梯度\n",
    "h = z ** 2\n",
    "y, z, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(112.), tensor(224.), tensor([   0.,  896., 1792., 2688.]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 调用反向传播函数\n",
    "h.backward()  # 求导\n",
    "z.grad, y.grad, x.grad  # 求完之后就可以通过_.grad来访问导数了\n",
    "# z.grad 中，dh/dz = 2 * z, 而z = 2y = 56, 所以z.grad = 2 * 56 = 112\n",
    "# y.grad 中，dz/dy = 2, 所以y.grad = z.grad * dz/dy = 112 * 2 = 224\n",
    "# x.grad 中，dy/dx = 4x, 所以x.grad = y.grad * dy/dx = 224 * 4x = [0, 896, 1792, 2688]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(True), tensor(True), tensor([True, True, True, True]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.grad == 2 * z, y.grad == z.grad * 2, x.grad == y.grad * 4 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在默认情况下，PyTorch 会累积梯度，我们需要清除之前的值\n",
    "x.grad.zero_()\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.], requires_grad=True),\n",
       " tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>),\n",
       " tensor([ 0.,  3., 12., 27.]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对非标量调用`backward`需要传入一个`gradient`参数，该参数指定微分函数关于`self`的梯度。\n",
    "# 在我们的例子中，我们只想求导数的和，所以传递一个1的梯度是合适的\n",
    "x.grad.zero_()\n",
    "y = x * x * x\n",
    "# 等价于y.backward(torch.ones(len(x)))\n",
    "y.sum().backward()  # 这里y.sum()是标量, 此时y.sum() = x0^3 + x1^3 + x2^3 + x3^3, 所以y.sum().backward() = dy/dx = [3x0^2, 3x1^2, 3x2^2, 3x3^2]\n",
    "x, x * x, x.grad\n",
    "# 深度学习中一般对损失函数求导，而损失函数是标量，所以一般都是对标量调用backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将某些计算移动到记录的计算图之外\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()  # 将y的计算结果当成一个常数，赋值给u，此时u不再是关于x的函数，而是一个常数, u = [0, 1, 4, 9]\n",
    "z = u * x \n",
    "z.sum().backward()  # dz/dx = u = [0, 1, 4, 9]\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 作个简单对比\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "y.sum().backward()  # dy/dx = 2x\n",
    "x.grad == 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.4223, requires_grad=True), tensor(True))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 即使构建函数的计算图需要通过 Python 控制流（如条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度\n",
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c\n",
    "a = torch.randn(size=(), requires_grad=True)\n",
    "d = f(a)\n",
    "d.backward()\n",
    "a, a.grad == d / a\n",
    "# 在函数 f 中，b 是通过不断乘以 2 得到的，直到其范数大于等于 1000。\n",
    "# 假设 a 是一个标量，经过若干次乘以 2 后，b 可以表示为 b = a * 2^n，其中 n 是使得 b.norm() >= 1000 的最小整数\n",
    "# 在c = b的情况下，d = f(a) = c = a * 2^n\n",
    "# 在c = 100 * b的情况下，d = f(a) = c = 100 * a * 2^n\n",
    "# 无论哪种情况，d = f(a) = c都是a的线性函数，所以df(a)/da = 2^n或者2^n * 100 = d / a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor(1., grad_fn=<PowBackward0>) True\n",
      "tensor(1.) False\n",
      "tensor(2., grad_fn=<AddBackward0>) True\n"
     ]
    }
   ],
   "source": [
    "# 再来看看中断梯度追踪\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y1 = x ** 2 \n",
    "with torch.no_grad():  # 上下文管理器，不追踪梯度，常用于测试集\n",
    "    y2 = x ** 3\n",
    "y3 = y1 + y2\n",
    "\n",
    "print(x.requires_grad)\n",
    "print(y1, y1.requires_grad) # True\n",
    "print(y2, y2.requires_grad) # False\n",
    "print(y3, y3.requires_grad) # True\n",
    "# 可以看到，上面的y2是没有grad_fn而且y2.requires_grad=False的，而y3是有grad_fn的，y3.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3.backward()\n",
    "x.grad == 2 * x\n",
    "# 由于 y2 的定义是被torch.no_grad():包裹的，所以与 y2 有关的梯度是不会回传的，\n",
    "# 只有与 y1 有关的梯度才会回传，即 x^2 对 x 的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "False\n",
      "tensor([100.], requires_grad=True)\n",
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "# 此外，如果我们想要修改tensor的数值，但是又不希望被autograd记录（即不会影响反向传播），那么我么可以对tensor.data进行操作\n",
    "x = torch.ones(1,requires_grad=True)\n",
    "\n",
    "print(x.data) # 还是一个tensor\n",
    "print(x.data.requires_grad) # 但是已经是独立于计算图之外\n",
    "\n",
    "y = 2 * x\n",
    "x.data *= 100 # 只改变了值，不会记录在计算图，所以不会影响梯度传播\n",
    "\n",
    "y.backward()\n",
    "print(x) # 更改data的值也会影响tensor的值\n",
    "print(x.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
