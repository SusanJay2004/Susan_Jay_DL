{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在本节中，我们将介绍如何使用PyTorch更方便地实现线性回归的训练\n",
    "import torch\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from d2lzh_pytorch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2.1 生成数据集\n",
    "# 我们生成与上一节中相同的数据集。其中features是训练数据特征，labels是标签\n",
    "num_inputs = 2\n",
    "num_examples = 1000\n",
    "true_w = [2, -3.4]\n",
    "true_b = 4.2\n",
    "features = torch.tensor(np.random.normal(0, 1, (num_examples, num_inputs)), dtype=torch.float)\n",
    "labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\n",
    "labels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2.2 读取数据\n",
    "# PyTorch提供了data包来读取数据。由于data常用作变量名，我们将导入的data模块用Data代替。\n",
    "# 在每一次迭代中，我们将随机读取包含10个数据样本的小批量\n",
    "import torch.utils.data as Data\n",
    "\n",
    "batch_size = 10\n",
    "# 将训练数据的特征和标签组合\n",
    "dataset = Data.TensorDataset(features, labels)\n",
    "# 随机读取小批量\n",
    "data_iter = Data.DataLoader(dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5615, -0.5182],\n",
      "        [-1.1815, -0.1734],\n",
      "        [-1.0841, -1.1879],\n",
      "        [-0.1489,  2.1446],\n",
      "        [-0.9210, -0.9033],\n",
      "        [ 2.1355, -1.3205],\n",
      "        [ 1.9058,  0.8325],\n",
      "        [-0.1821,  2.3559],\n",
      "        [-0.2980, -0.6898],\n",
      "        [-1.5194,  0.2645]]) tensor([ 9.0879,  2.4307,  6.0738, -3.3944,  5.4255, 12.9639,  5.1703, -4.1703,\n",
      "         5.9521,  0.2583])\n"
     ]
    }
   ],
   "source": [
    "# 这里data_iter的使用跟上一节中的一样。让我们读取并打印第一个小批量数据样本\n",
    "for X, y in data_iter:\n",
    "    print(X, y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearNet(\n",
      "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 3.2.3 定义模型\n",
    "# 首先，导入torch.nn模块。实际上，“nn”是neural networks（神经网络）的缩写。\n",
    "# 顾名思义，该模块定义了大量神经网络的层。之前我们已经用过了autograd，而nn就是利用autograd来定义模型。\n",
    "# nn的核心数据结构是Module，它是一个抽象概念，既可以表示神经网络中的某个层（layer），也可以表示一个包含很多层的神经网络。\n",
    "# 在实际使用中，最常见的做法是继承nn.Module，撰写自己的网络/层。\n",
    "# 一个nn.Module实例应该包含一些层以及返回输出的前向传播（forward）方法。\n",
    "# 下面先来看看如何用nn.Module实现一个线性回归模型\n",
    "\n",
    "class LinearNet(nn.Module):   \n",
    "    # 定义一个名为 LinearNet 的新类，继承自 PyTorch 的 nn.Module，\n",
    "    # nn.Module 是 PyTorch 中所有神经网络模块的基类，提供了参数管理、设备迁移等基础功能\n",
    "    def __init__(self, n_feature):\n",
    "        # 定义类的初始化方法\n",
    "        # 接收一个参数 n_feature，表示输入特征的维度（即有多少个特征）\n",
    "\n",
    "        # 调用父类的初始化方法，确保nn.Module的初始化方法被正确调用，第一个参数告诉Python从LinearNet的父类中查找__init__方法\n",
    "        super(LinearNet, self).__init__()  \n",
    "\n",
    "        # 创建一个线性层（全连接层）对象，\n",
    "        # nn.Linear(n_feature, 1) 表示一个从 n_feature 维到 1 维的线性变换：y = Wx + b\n",
    "        # 将这个线性层保存为类的属性 self.linear\n",
    "        # 这里输出维度为 1，因为线性回归预测的是一个标量值\n",
    "        self.linear = nn.Linear(n_feature, 1)\n",
    "\n",
    "    # forward 定义前向传播：\n",
    "    # 在 PyTorch 中，必须实现 forward 方法来定义计算逻辑\n",
    "    # 接收输入数据 x，返回模型的输出 y\n",
    "    def forward(self, x):\n",
    "        # 将输入 x 传入线性层进行计算\n",
    "        # self.linear(x) 等价于执行 y = Wx + b 的线性变换\n",
    "        # 结果保存在变量 y 中\n",
    "        y = self.linear(x)\n",
    "        return y\n",
    "\n",
    "# 创建 LinearNet 类的一个实例，命名为 net\n",
    "# num_inputs 是一个之前定义的变量(值为2)，表示输入特征的数量\n",
    "net = LinearNet(num_inputs)\n",
    "print(net) # 使用print可以打印出网络的结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事实上我们还可以用nn.Sequential来更加方便地搭建网络，Sequential是一个有序的容器，网络层将按照在传入Sequential的顺序依次被添加到计算图中。\n",
    "\n",
    "写法一\n",
    "\n",
    "net = nn.Sequential(\n",
    "\n",
    "          nn.Linear(num_inputs, 1)\n",
    "\n",
    "          # 此处还可以传入其他层\n",
    "\n",
    "          )\n",
    "\n",
    "写法二\n",
    "\n",
    "net = nn.Sequential()\n",
    "\n",
    "net.add_module('linear', nn.Linear(num_inputs, 1))\n",
    "\n",
    "#net.add_module ......\n",
    "\n",
    "写法三\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "net = nn.Sequential(OrderedDict([\n",
    "\n",
    "            ('linear', nn.Linear(num_inputs, 1))\n",
    "\n",
    "            # ......\n",
    "\n",
    "            ]))\n",
    "\n",
    "\n",
    "print(net)\n",
    "\n",
    "print(net[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0211,  0.2165]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.4958], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 可以通过net.parameters()来查看模型所有的可学习参数，此函数将返回一个生成器\n",
    "for param in net.parameters():\n",
    "    print(param)\n",
    "# 回顾图3.1中线性回归在神经网络图中的表示。\n",
    "# 作为一个单层神经网络，线性回归输出层中的神经元和输入层中各个输入完全连接。因此，线性回归的输出层又叫全连接层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：torch.nn仅支持输入一个batch的样本，不支持单个样本输入，如果只有单个样本，可使用input.unsqueeze(0)来添加一维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我想问一下，我还没有定义 w1, w2, b 这几个权重和偏差，为什么可以打印出可学习参数的具体数字？\n",
    "\n",
    "GitHub Copilot：\n",
    "这是一个很好的问题！\n",
    "\n",
    "在PyTorch中，当你创建网络层时（如nn.Linear），即使你没有显式定义权重和偏置，PyTorch也会自动为你创建并初始化这些参数。\n",
    "\n",
    "自动参数初始化机制\n",
    "当你创建nn.Linear(num_inputs, 1)时，以下过程自动发生：\n",
    "\n",
    "参数创建：\n",
    "\n",
    "PyTorch自动创建权重矩阵（形状为[1, num_inputs]）\n",
    "自动创建偏置向量（形状为[1]）\n",
    "\n",
    "默认初始化：\n",
    "权重使用何凯明初始化（Kaiming/He initialization）或均匀分布初始化（取决于PyTorch版本）\n",
    "偏置通常初始化为零\n",
    "\n",
    "参数注册：\n",
    "这些参数自动注册为模型的可学习参数\n",
    "它们被存储在模型的parameters()集合中\n",
    "\n",
    "# 访问方式：\n",
    "\n",
    "# 如果使用Sequential\n",
    "\n",
    "weight = net[0].weight  # 访问第一层的权重\n",
    "\n",
    "bias = net[0].bias      # 访问第一层的偏置\n",
    "\n",
    "# 如果使用自定义LinearNet类\n",
    "\n",
    "weight = net.linear.weight  # 访问linear层的权重\n",
    "\n",
    "bias = net.linear.bias      # 访问linear层的偏置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.], requires_grad=True)"
      ]
     },
     "execution_count": 851,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.3.4 初始化模型参数\n",
    "# 在使用net前，我们需要初始化模型参数，如线性回归模型中的权重和偏差。\n",
    "# PyTorch在init模块中提供了多种参数初始化方法\n",
    "# 这里的init是initializer的缩写形式\n",
    "# 我们通过init.normal_将权重参数每个元素初始化为随机采样于均值为0、标准差为0.01的正态分布。偏差会初始化为零\n",
    "from torch.nn import init\n",
    "\n",
    "# normal_使用正态分布（高斯分布）随机数初始化权重参数\n",
    "init.normal_(net.linear.weight, mean=0, std=0.01)\n",
    "# constant_将张量的所有元素设置为同一个常数值\n",
    "init.constant_(net.linear.bias, val=0)  \n",
    "# 也可以直接修改bias的data: net[0].bias.data.fill_(0), 但通常推荐使用init模块的函数，因为它们提供了更统一的接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3.5 定义损失函数\n",
    "# PyTorch在nn模块中提供了各种损失函数，这些损失函数可看作是一种特殊的层，PyTorch也将这些损失函数实现为nn.Module的子类\n",
    "# 我们现在使用它提供的均方误差损失作为模型的损失函数\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.03\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 3.3.6 定义优化算法\n",
    "# 同样，我们也无须自己实现小批量随机梯度下降算法。torch.optim模块提供了很多常用的优化算法比如SGD、Adam和RMSProp等\n",
    "# 下面我们创建一个用于优化net所有参数的优化器实例，并指定学习率为0.03的小批量随机梯度下降（SGD）为优化算法\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.03)\n",
    "print(optimizer)\n",
    "\n",
    "# 多参数组示例：\n",
    "# optimizer = optim.SGD([\n",
    "#     {'params': [net.linear.weight]}, # 子网络1的参数使用较大的学习率\n",
    "#     {'params': [net.linear.bias], 'lr': 0.01} # 子网络2的参数使用较小的学习率\n",
    "# ], lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们还可以为不同子网络设置不同的学习率，这在finetune时经常用到。例：\n",
    "# optimizer =optim.SGD([\n",
    "#                 # 如果对某个参数不指定学习率，就使用最外层的默认学习率\n",
    "#                 {'params': net.subnet1.parameters()}, # lr=0.03\n",
    "#                 {'params': net.subnet2.parameters(), 'lr': 0.01}\n",
    "#             ], lr=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有时候我们不想让学习率固定成一个常数，那如何调整学习率呢？主要有两种做法。一种是修改optimizer.param_groups中对应的学习率，另一种是更简单也是较为推荐的做法——新建优化器，由于optimizer十分轻量级，构建开销很小，故而可以构建新的optimizer。但是后者对于使用动量的优化器（如Adam），会丢失动量等状态信息，可能会造成损失函数的收敛出现震荡等情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调整学习率\n",
    "# 在优化器的 param_groups 中，\"group\" 与您的样本数量和批次大小无关，它指的是模型参数的分组，而不是数据样本的分组\n",
    "\n",
    "# for param_group in optimizer.param_groups:\n",
    "#     param_group['lr'] *= 0.1 # 学习率为之前的0.1倍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 0.000176\n",
      "epoch 2, loss: 0.000173\n",
      "epoch 3, loss: 0.000071\n",
      "epoch 4, loss: 0.000063\n",
      "epoch 5, loss: 0.000182\n",
      "epoch 6, loss: 0.000068\n",
      "epoch 7, loss: 0.000047\n",
      "epoch 8, loss: 0.000112\n",
      "epoch 9, loss: 0.000119\n",
      "epoch 10, loss: 0.000146\n",
      "epoch 11, loss: 0.000177\n",
      "epoch 12, loss: 0.000066\n",
      "epoch 13, loss: 0.000097\n",
      "epoch 14, loss: 0.000144\n",
      "epoch 15, loss: 0.000105\n",
      "epoch 16, loss: 0.000072\n",
      "epoch 17, loss: 0.000039\n",
      "epoch 18, loss: 0.000080\n",
      "epoch 19, loss: 0.000152\n",
      "epoch 20, loss: 0.000035\n"
     ]
    }
   ],
   "source": [
    "# 3.3.7 训练模型\n",
    "# 在使用Gluon训练模型时，我们通过调用optim实例的step函数来迭代模型参数\n",
    "# 按照小批量随机梯度下降的定义，我们在step函数中指明批量大小，从而对批量中样本梯度求平均\n",
    "num_epochs = 20\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for X, y in data_iter:\n",
    "        output = net(X)   \n",
    "        # 功能：模型前向传播，计算预测值\n",
    "        # 过程：\n",
    "        # 将特征数据 X 输入模型\n",
    "        # 调用模型的 forward 方法（隐式调用）\n",
    "        # net(X) 等价于 net.forward(X)\n",
    "        # 结果：\n",
    "        # output 包含模型对当前批次的预测结果\n",
    "        # 对于线性回归，形状为 [batch_size, 1]\n",
    "\n",
    "        # 功能：计算损失函数值；y.view(-1, 1): 将标签重塑为列向量，确保标签形状与输出一致\n",
    "        l = loss(output, y.view(-1, 1))   \n",
    "\n",
    "        # 梯度清零，等价于net.zero_grad()，区别在于 optimizer.zero_grad() 只清零优化器中的参数梯度\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        # 功能：执行反向传播，计算梯度\n",
    "        # 过程：\n",
    "        # 从损失 l 开始，向后计算计算图中每个参数的梯度\n",
    "        # 使用自动微分计算链式法则\n",
    "        l.backward()\n",
    "\n",
    "        # 功能：根据梯度更新模型参数\n",
    "        # 过程：\n",
    "        # 使用预设的优化算法（如SGD）更新参数\n",
    "        # 参数更新公式：param = param - learning_rate * param.grad / batch_size\n",
    "        # 结果：\n",
    "        # 模型参数向着减小损失的方向移动\n",
    "        # 这一步完成后，模型对当前批次数据的拟合能力提升\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= 0.7 # 学习率为之前的0.7倍(实际上调整学习率有专门的函数)\n",
    "    # 实际应用中通常使用更温和的策略，如每几个epoch才衰减一次，或使用更小的衰减系数\n",
    "    \n",
    "    print('epoch %d, loss: %f' % (epoch, l.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, -3.4] Parameter containing:\n",
      "tensor([[ 1.9999, -3.3997]], requires_grad=True)\n",
      "4.2 Parameter containing:\n",
      "tensor([4.2001], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 下面我们分别比较学到的模型参数和真实的模型参数。\n",
    "# 我们从net获得需要的层，并访问其权重（weight）和偏差（bias）。学到的参数和真实的参数很接近\n",
    "dense = net.linear\n",
    "print(true_w, dense.weight)\n",
    "print(true_b, dense.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 小结\n",
    "使用PyTorch可以更简洁地实现模型。\n",
    "\n",
    "torch.utils.data模块提供了有关数据处理的工具，torch.nn模块定义了大量神经网络的层，torch.nn.init模块定义了各种初始化方法，torch.optim模块提供了很多常用的优化算法。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
